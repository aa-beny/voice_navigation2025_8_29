# -*- coding: utf-8 -*-
# æª”æ¡ˆï¼švoice_nav2_assistant/voice_nav_node.py
# åŠŸèƒ½ï¼šè®€ WAV æª” â†’ Whisper(èªéŸ³è¾¨è­˜) â†’ï¼ˆLLM/è¦å‰‡ï¼šç†è§£å»å“ªè£¡ï¼‰â†’ æŸ¥ places.yaml â†’ Nav2 å°èˆª

import os, math, json, glob, yaml                # ä½œæ¥­ç³»çµ±/æ•¸å­¸/JSON/è¬ç”¨å­—å…ƒæ‰¾æª”/è®€ YAML
from typing import Dict
from dataclasses import dataclass               # è®“æˆ‘å€‘ç”¨ç°¡å–®çš„è³‡æ–™çµæ§‹å­˜åº§æ¨™

import rclpy                                    # ROS 2 Python å®¢æˆ¶ç«¯
from rclpy.node import Node
from rclpy.action import ActionClient           # é€ Nav2 çš„ action ç”¨

from nav2_msgs.action import NavigateToPose     # Nav2 çš„ç›®æ¨™è¨Šæ¯æ ¼å¼
from geometry_msgs.msg import PoseStamped, Quaternion

from faster_whisper import WhisperModel         # æœ¬åœ° Whisperï¼ˆæŠŠè²éŸ³â†’æ–‡å­—ï¼‰

import subprocess, tempfile

# ====ï¼ˆå¯é¸ï¼‰OpenAIï¼šåªæœ‰ä½ è¦ç”¨ GPT æ‰æœƒè¼‰====
OPENAI_OK = False
try:
    from openai import OpenAI                   # å®˜æ–¹ SDK
    OPENAI_OK = True if os.getenv("OPENAI_API_KEY") else False
except Exception:
    OPENAI_OK = False


def yaw_to_quat(yaw: float) -> Quaternion:
    """æŠŠå¹³é¢ä¸Šçš„æœå‘è§’ (yaw, å¼§åº¦) è½‰æˆå››å…ƒæ•¸(ROS éœ€è¦çš„æ ¼å¼)"""
    q = Quaternion()
    half = yaw / 2.0
    q.z = math.sin(half)
    q.w = math.cos(half)
    q.x = 0.0
    q.y = 0.0
    return q


@dataclass
class Place:
    """å­˜ä¸€å€‹åœ°é»ï¼šx, y, yawï¼ˆå¾ places.yaml ä¾†ï¼‰"""
    x: float
    y: float
    yaw: float


class VoiceNavNode(Node):
    """æ•´å€‹èªéŸ³å°èˆªç¯€é»ï¼šå•Ÿå‹•æ™‚å°±æœƒæŠŠæ‰€æœ‰éŸ³æª”è·‘ä¸€é"""

    def __init__(self):
        super().__init__("voice_nav_node")

        # ---- å®£å‘Š ROS åƒæ•¸ï¼ˆå¯ä»¥å¾ launch æˆ–å‘½ä»¤åˆ—å¸¶é€²ä¾†ï¼‰----
        self.declare_parameter("places_file", "")            # places.yaml è·¯å¾‘
        self.declare_parameter("audio_mode", "file")         # file | micï¼ˆç¾åœ¨ç”¨ fileï¼‰
        self.declare_parameter("audio_files", "")            # èªéŸ³æª”æ¸…å–®ï¼ˆé€—è™Ÿæˆ–è¬ç”¨å­—å…ƒï¼‰
        self.declare_parameter("asr_model_size", "small")    # Whisper å°ºå¯¸ï¼štiny/base/small/medium
        self.declare_parameter("use_openai", False)          # è¦ä¸è¦ç”¨ GPT
        self.declare_parameter("openai_model", "gpt-4o-mini")

        # ---- è®€å–åƒæ•¸å€¼ ----
        self.places_file = self.get_parameter("places_file").get_parameter_value().string_value
        self.audio_mode  = self.get_parameter("audio_mode").get_parameter_value().string_value
        self.audio_files_pat = self.get_parameter("audio_files").get_parameter_value().string_value
        self.asr_model_size = self.get_parameter("asr_model_size").get_parameter_value().string_value
        self.use_openai = self.get_parameter("use_openai").get_parameter_value().bool_value
        self.openai_model = self.get_parameter("openai_model").get_parameter_value().string_value


        self.get_logger().info(
        f"Params: places_file={self.places_file}, audio_mode={self.audio_mode}, "
        f"asr_model={self.asr_model_size}, use_openai={self.use_openai}, model={self.openai_model}"
)

        # ---- è®€ places.yaml è®Šæˆå­—å…¸ ----
        #    ä¾‹å¦‚ snack_bin: {x: 3.12, y: -1.45, yaw: 1.57}
        self.places: Dict[str, Place] = {}
        with open(self.places_file, "r") as f:
            raw = yaml.safe_load(f)
        for k, v in raw.items():
            self.places[k] = Place(float(v["x"]), float(v["y"]), float(v["yaw"]))
        self.get_logger().info(f"Loaded places: {list(self.places.keys())}")

        for name, p in self.places.items():
            self.get_logger().info(f"Place loaded: {name} -> x={p.x:.2f}, y={p.y:.2f}, yaw={p.yaw:.3f}rad")

        # ---- æº–å‚™ ASRï¼ˆèªéŸ³â†’æ–‡å­—ï¼‰----
        self.get_logger().info(f"Loading faster-whisper model: {self.asr_model_size}")
        # device="auto"ï¼šCUDA æœ‰å°±ç”¨ GPUï¼›compute_type="int8_float16"ï¼šåœ¨ Jetson ä¸Šçœè¨˜æ†¶é«”
        # self.whisper = WhisperModel(self.asr_model_size, device="auto", compute_type="int8_float16")
        self.whisper = WhisperModel(self.asr_model_size, device="cpu", compute_type="int8")


        # ---- ï¼ˆé¸ç”¨ï¼‰æº–å‚™ LLM ----
        self.llm = None
        if self.use_openai:
            if not OPENAI_OK:
                self.get_logger().error("use_openai=True ä½† OPENAI_API_KEY æœªè¨­å®šã€‚")
            else:
                try:
                    self.llm = OpenAI()  # ä¹‹å¾Œå‘¼å« chat.completions ç”¨
                    self.get_logger().info(f"OpenAI model: {self.openai_model}")
                except Exception as e:
                    self.get_logger().error(f"OpenAI åˆå§‹åŒ–å¤±æ•—ï¼š{e}")

        # ---- Nav2 Action Clientï¼ˆæŠŠç›®æ¨™ Pose ä¸Ÿçµ¦ Nav2ï¼‰----
        self.nav_client = ActionClient(self, NavigateToPose, "navigate_to_pose")
        self.get_logger().info("Waiting for Nav2 action serverâ€¦")
        self.nav_client.wait_for_server()                 # ç­‰ Nav2 æº–å‚™å¥½
        self.get_logger().info("Nav2 action server ready.")

        # ---- ç¾åœ¨æ˜¯ã€Œæª”æ¡ˆæ¨¡å¼ã€ï¼Œå•Ÿå‹•å¾Œå°±æŠŠéŸ³æª”ä¸€æ¬¡è™•ç†å®Œ ----
        if self.audio_mode == "file":
            self.get_logger().info("ğŸ—‚ï¸ ä½¿ç”¨æª”æ¡ˆæ¨¡å¼ (audio_mode=file)")
            self.process_audio_files()
        else:
            self.loop_mic()

    # ============= è®€å…¥æ‰€æœ‰èªéŸ³æª” â†’ é€ä¸€è™•ç† =============
    def process_audio_files(self):
        self.get_logger().info(f"Audio patterns: {self.audio_files_pat}")
        paths = []
        if not self.audio_files_pat:
            self.get_logger().error("audio_files åƒæ•¸æœªæŒ‡å®šã€‚")
            return

        # æŠŠå­—ä¸²æ‹†æˆå¤šå€‹ patternï¼ˆé€—è™Ÿåˆ†éš”ï¼›å¯ç”¨è¬ç”¨å­—å…ƒ *.wavï¼‰
        for token in self.audio_files_pat.split(","):
            token = token.strip()
            if not token:
                continue
            # glob æœƒæŠŠ "~/voice_tests/*.wav" å±•é–‹æˆçœŸæ­£çš„æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
            paths.extend(sorted(glob.glob(os.path.expanduser(token))))

        self.get_logger().info(f"Found {len(paths)} audio files")
        if not paths:
            self.get_logger().error(f"æ‰¾ä¸åˆ°éŸ³æª”ï¼š{self.audio_files_pat}")
            return

        # é€å€‹æª”æ¡ˆï¼šASRâ†’æ–‡å­—â†’ç†è§£å»å“ªâ†’æŸ¥åº§æ¨™â†’è«‹ Nav2 èµ°
        for p in paths:
            self.get_logger().info(f"[ASR] è½‰éŒ„éŸ³æª”ï¼š{p}")
            text = self.transcribe_file(p)                # Whisper è½‰æˆæ–‡å­—
            self.get_logger().info(f"[ASR] æ–‡å­—ï¼š{text}")
            if not text:
                continue

            place_id = self.infer_place(text)             # è®€æ‡‚ã€Œè¦å»å“ªã€
            if not place_id:
                self.get_logger().warn("è§£æä¸åˆ°ç›®çš„åœ°ï¼Œç•¥éæ­¤æª”ã€‚")
                continue
            if place_id not in self.places:
                self.get_logger().warn(f"æœªçŸ¥åœ°é»ï¼š{place_id}ï¼Œè«‹åŠ å…¥ places.yaml")
                continue

            self.navigate_to(place_id)                    # çœŸçš„é€ Nav2

        self.get_logger().info("æ‰€æœ‰éŸ³æª”è™•ç†å®Œæˆã€‚")

    # ============= Whisper æŠŠèªéŸ³æª”è½‰æˆæ–‡å­— =============
    def transcribe_file(self, wav_path: str) -> str:
        try:
            # language="zh" å‘Šè¨´å®ƒç”¨ä¸­æ–‡æ¨¡å‹è§£ï¼›beam_size=1 é€Ÿåº¦å¿«ä¸€é»
            segments, info = self.whisper.transcribe(
                wav_path, language="zh", vad_filter=False, beam_size=1
            )
            # Whisper æœƒç”¢ç”Ÿå¤šæ®µå­—ä¸²ï¼Œé€™è£¡æŠŠå®ƒå€‘æ¥èµ·ä¾†
            out = "".join([seg.text for seg in segments]).strip()
            return out
        except Exception as e:
            self.get_logger().error(f"Whisper è½‰éŒ„å¤±æ•—ï¼š{e}")
            return ""

    # ============= è®€æ‡‚ä½ è¦å»å“ªï¼ˆæ„åœ–è§£æï¼‰=============
    def infer_place(self, text: str) -> str:
        self.get_logger().info(f"[NLU] input: {text}")
        # å…ˆå˜—è©¦ç”¨ GPTï¼ˆå¦‚æœä½ æœ‰é–‹ use_openai ä¸”æœ‰é‡‘é‘°ï¼‰
        if self.use_openai and self.llm is not None:
            try:
                places = list(self.places.keys())
                # è¨­è¨ˆä¸€å€‹ã€Œåªå›å‚³ JSONã€çš„æç¤ºè©ï¼Œè®“ GPT å› {"place_id":"xxx"}
                prompt = (
                    "ä½ æ˜¯æ©Ÿå™¨äººä»»å‹™è§£æå™¨ã€‚è¼¸å…¥æ˜¯ä¸€å¥ä¸­æ–‡å£èªï¼Œ"
                    "è«‹åªå›å‚³ JSONï¼Œæ¬„ä½åªæœ‰ place_idã€‚"
                    f"å¯é¸åœ°é»ï¼š{places}\n"
                    "è‹¥èƒ½å°æ‡‰ï¼Œè¼¸å‡ºå¦‚ {\"place_id\":\"snack_bin\"}ï¼›"
                    "è‹¥ç„¡æ³•å°æ‡‰ï¼Œè¼¸å‡º {\"place_id\":\"\"}ã€‚\n"
                    f"æŒ‡ä»¤ï¼šã€Œ{text}ã€"
                )
                resp = self.llm.chat.completions.create(
                    model=self.openai_model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.0,                    # ä¸è¦ç™¼æ®å‰µæ„ï¼Œç…§æº–ç¢ºé…å°
                )
                content = resp.choices[0].message.content
                self.get_logger().info(f"[NLU/GPT] raw: {content}")
                data = json.loads(content)               # è§£æ JSON å­—ä¸²
                return data.get("place_id", "")
            except Exception as e:
                # GPT å£äº†å°±é€€å›é›¢ç·šè¦å‰‡
                self.get_logger().warn(f"OpenAI è§£æå¤±æ•—ï¼š{e}ï¼Œæ”¹ç”¨é›¢ç·šè¦å‰‡ã€‚")

        # ---- é›¢ç·šè¦å‰‡ï¼šä¸é ç¶²è·¯ã€çœ‹é—œéµå­— ----
        kw = {
            "é›¶é£Ÿ": "snack_bin",
            "é›¶é£Ÿæ«ƒ": "snack_bin",
            "é¤…ä¹¾": "snack_bin",
            "é£²æ–™": "snack_bin",
            "å»šæˆ¿": "kitchen_table",
            "æ¡Œå­": "kitchen_table",
            "æœƒè­°å®¤": "meeting_room",
        }
        # æœ‰å°åˆ°é—œéµå­—å°±å›å°æ‡‰åœ°é»
        for k, pid in kw.items():
            if k in text:
                return pid

        # ä½ ç›´æ¥èªªåœ°é»ä»£è™Ÿï¼ˆä¾‹å¦‚ "snack_bin"ï¼‰ä¹Ÿæ¥å—
        for pid in self.places.keys():
            if pid in text:
                return pid

        # ä¸€äº›å¸¸è¦‹å¥å‹è£œå¼·
        if "åƒ" in text and ("é›¶é£Ÿ" in text or "é¤…ä¹¾" in text or "é£²æ–™" in text):
            return "snack_bin"
        if ("å»" in text or "åˆ°" in text) and "æ¡Œ" in text:
            return "kitchen_table"
        return ""                                     # çœŸçš„è½ä¸æ‡‚å°±å›ç©ºå­—ä¸²

    # ============= é€å°èˆªç›®æ¨™çµ¦ Nav2 =============
    def navigate_to(self, place_id: str):
        self.get_logger().info("Sending goal to Nav2â€¦")
        p = self.places[place_id]                    # å–å‡º (x,y,yaw)
        goal = NavigateToPose.Goal()
        pose = PoseStamped()
        pose.header.frame_id = "map"                 # ä»¥åœ°åœ–åº§æ¨™ç‚ºåŸºæº–
        pose.header.stamp = self.get_clock().now().to_msg()
        pose.pose.position.x = p.x
        pose.pose.position.y = p.y
        pose.pose.position.z = 0.0
        pose.pose.orientation = yaw_to_quat(p.yaw)   # æœå‘è½‰æˆå››å…ƒæ•¸
        goal.pose = pose

        self.get_logger().info(
            f"å°èˆª â†’ {place_id}: ({p.x:.2f},{p.y:.2f}, yaw={p.yaw:.2f})"
        )

        # éåŒæ­¥é€å‡ºç›®æ¨™ï¼Œä¸¦ç­‰å¾…çµæœ
        send_future = self.nav_client.send_goal_async(goal)
        rclpy.spin_until_future_complete(self, send_future)
        gh = send_future.result()
        if gh and gh.accepted:
            self.get_logger().info("Nav2 å·²æ¥å—ç›®æ¨™ã€‚")
        if not gh or not gh.accepted:
            self.get_logger().error("ç›®æ¨™è¢« Nav2 æ‹’çµ•ã€‚")
            return

        res_future = gh.get_result_async()
        rclpy.spin_until_future_complete(self, res_future)
        status = res_future.result().status
        if status == 4:
            self.get_logger().info("âœ… å·²åˆ°é”ç›®æ¨™ã€‚")      # Nav2 å›å ±æˆåŠŸçš„ä»£è™Ÿæ˜¯ 4
        else:
            self.get_logger().warn(f"âš ï¸ å°èˆªæœªæˆåŠŸï¼Œç‹€æ…‹={status}")
    def loop_mic(self):
        """æŒ‰ Enter éŒ„ 3 ç§’ â†’ Whisper â†’ è§£æ â†’ Nav2ã€‚q + Enter é›¢é–‹ã€‚"""
        dur = int(os.environ.get('MIC_DURATION', '3'))   # éŒ„éŸ³ç§’æ•¸
        rate = int(os.environ.get('MIC_RATE', '16000'))  # å–æ¨£ç‡
        self.get_logger().info("ğŸ¤ éº¥å…‹é¢¨æ¨¡å¼ï¼šæŒ‰ Enter é–‹å§‹éŒ„éŸ³ 3 ç§’ï¼Œè¼¸å…¥ q + Enter é›¢é–‹")
        while rclpy.ok():
            try:
                line = input()
            except EOFError:
                break
            if line.strip().lower() == 'q':
                break
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                wav = tmp.name
            # éŒ„å–®è²é“ 16k wavï¼ˆJetson ä¸Šæœ€ç©©ï¼‰
            cmd = ['arecord','-q','-d',str(dur),'-f','S16_LE','-r',str(rate),'-c','1', wav]
            subprocess.run(cmd, check=False)
            text = self.transcribe_file(wav)
            self.get_logger().info(f"[ASR/MIC] æ–‡å­—ï¼š{text}")
            if not text:
                continue
            place_id = self.infer_place(text)
            if place_id and place_id in self.places:
                self.navigate_to(place_id)
            else:
                self.get_logger().warn("è½ä¸æ‡‚æˆ–æœªçŸ¥åœ°é»ï¼Œè«‹å†èªªä¸€æ¬¡ã€‚")


def main():
    rclpy.init()               # å•Ÿå‹• ROS
    node = VoiceNavNode()      # å»ºç«‹ä¸¦åŸ·è¡Œä¸Šé¢æµç¨‹
    rclpy.try_shutdown()       # åšå®Œæ”¶æ”¤


if __name__ == "__main__":
    main()

